{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ssabu2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ssabu2/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ssabu2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ssabu2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from textprep import text_prep\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Lambda\n",
    "from gensim.models import FastText\n",
    "import json\n",
    "import pickle\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(852, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>GT</th>\n",
       "      <th>Text</th>\n",
       "      <th>Created_At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.317271e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>left my glasses at a restaurant and called the...</td>\n",
       "      <td>Sat Oct 17 01:05:47 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.317270e+18</td>\n",
       "      <td>N</td>\n",
       "      <td>I worked at a restaurant that was requiring th...</td>\n",
       "      <td>Sat Oct 17 01:03:17 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.317269e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>@testpatternshow Went to a local restaurant, &amp;...</td>\n",
       "      <td>Sat Oct 17 00:58:38 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.317269e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>OMG...went to pick up food at a restaurant tha...</td>\n",
       "      <td>Sat Oct 17 00:57:43 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.317266e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>Update from the Inner Cook County Resistance: ...</td>\n",
       "      <td>Sat Oct 17 00:49:17 +0000 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Tweet_ID GT  \\\n",
       "0         0.0  1.317271e+18  Y   \n",
       "1         1.0  1.317270e+18  N   \n",
       "2         2.0  1.317269e+18  Y   \n",
       "3         3.0  1.317269e+18  Y   \n",
       "4         4.0  1.317266e+18  Y   \n",
       "\n",
       "                                                Text  \\\n",
       "0  left my glasses at a restaurant and called the...   \n",
       "1  I worked at a restaurant that was requiring th...   \n",
       "2  @testpatternshow Went to a local restaurant, &...   \n",
       "3  OMG...went to pick up food at a restaurant tha...   \n",
       "4  Update from the Inner Cook County Resistance: ...   \n",
       "\n",
       "                       Created_At  \n",
       "0  Sat Oct 17 01:05:47 +0000 2020  \n",
       "1  Sat Oct 17 01:03:17 +0000 2020  \n",
       "2  Sat Oct 17 00:58:38 +0000 2020  \n",
       "3  Sat Oct 17 00:57:43 +0000 2020  \n",
       "4  Sat Oct 17 00:49:17 +0000 2020  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = pd.read_excel(\"tweets3.xlsx\")\n",
    "print(dataset1.shape)\n",
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1076, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>GT</th>\n",
       "      <th>Text</th>\n",
       "      <th>Created_At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.323697e+18</td>\n",
       "      <td>N</td>\n",
       "      <td>I do!!! When was the last time you went to an ...</td>\n",
       "      <td>Tue Nov 03 18:41:49 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.323689e+18</td>\n",
       "      <td>N</td>\n",
       "      <td>Hoped off the plane from Vermont the 18th of O...</td>\n",
       "      <td>Tue Nov 03 18:09:30 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.323677e+18</td>\n",
       "      <td>N</td>\n",
       "      <td>@runmyheart I once went to an art event where ...</td>\n",
       "      <td>Tue Nov 03 17:21:59 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.323670e+18</td>\n",
       "      <td>N</td>\n",
       "      <td>@AndHeGrows I went 30-11 on a juggernaut round...</td>\n",
       "      <td>Tue Nov 03 16:56:04 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.323661e+18</td>\n",
       "      <td>N</td>\n",
       "      <td>2016 I remember myself and some friends making...</td>\n",
       "      <td>Tue Nov 03 16:19:24 +0000 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Tweet_ID GT  \\\n",
       "0         0.0  1.323697e+18  N   \n",
       "1         1.0  1.323689e+18  N   \n",
       "2         2.0  1.323677e+18  N   \n",
       "3         3.0  1.323670e+18  N   \n",
       "4         4.0  1.323661e+18  N   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I do!!! When was the last time you went to an ...   \n",
       "1  Hoped off the plane from Vermont the 18th of O...   \n",
       "2  @runmyheart I once went to an art event where ...   \n",
       "3  @AndHeGrows I went 30-11 on a juggernaut round...   \n",
       "4  2016 I remember myself and some friends making...   \n",
       "\n",
       "                       Created_At  \n",
       "0  Tue Nov 03 18:41:49 +0000 2020  \n",
       "1  Tue Nov 03 18:09:30 +0000 2020  \n",
       "2  Tue Nov 03 17:21:59 +0000 2020  \n",
       "3  Tue Nov 03 16:56:04 +0000 2020  \n",
       "4  Tue Nov 03 16:19:24 +0000 2020  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = pd.read_excel(\"tweets4.xlsx\")\n",
    "print(dataset2.shape)\n",
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.concat([dataset1,dataset2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1928, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>GT</th>\n",
       "      <th>Text</th>\n",
       "      <th>Created_At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.317271e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>left my glasses at a restaurant and called the...</td>\n",
       "      <td>Sat Oct 17 01:05:47 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.317270e+18</td>\n",
       "      <td>N</td>\n",
       "      <td>I worked at a restaurant that was requiring th...</td>\n",
       "      <td>Sat Oct 17 01:03:17 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.317269e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>@testpatternshow Went to a local restaurant, &amp;...</td>\n",
       "      <td>Sat Oct 17 00:58:38 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.317269e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>OMG...went to pick up food at a restaurant tha...</td>\n",
       "      <td>Sat Oct 17 00:57:43 +0000 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.317266e+18</td>\n",
       "      <td>Y</td>\n",
       "      <td>Update from the Inner Cook County Resistance: ...</td>\n",
       "      <td>Sat Oct 17 00:49:17 +0000 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Tweet_ID GT  \\\n",
       "0         0.0  1.317271e+18  Y   \n",
       "1         1.0  1.317270e+18  N   \n",
       "2         2.0  1.317269e+18  Y   \n",
       "3         3.0  1.317269e+18  Y   \n",
       "4         4.0  1.317266e+18  Y   \n",
       "\n",
       "                                                Text  \\\n",
       "0  left my glasses at a restaurant and called the...   \n",
       "1  I worked at a restaurant that was requiring th...   \n",
       "2  @testpatternshow Went to a local restaurant, &...   \n",
       "3  OMG...went to pick up food at a restaurant tha...   \n",
       "4  Update from the Inner Cook County Resistance: ...   \n",
       "\n",
       "                       Created_At  \n",
       "0  Sat Oct 17 01:05:47 +0000 2020  \n",
       "1  Sat Oct 17 01:03:17 +0000 2020  \n",
       "2  Sat Oct 17 00:58:38 +0000 2020  \n",
       "3  Sat Oct 17 00:57:43 +0000 2020  \n",
       "4  Sat Oct 17 00:49:17 +0000 2020  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>left my glasses at a restaurant and called the...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I worked at a restaurant that was requiring th...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@testpatternshow Went to a local restaurant, &amp;...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OMG...went to pick up food at a restaurant tha...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Update from the Inner Cook County Resistance: ...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label\n",
       "0  left my glasses at a restaurant and called the...     Y\n",
       "1  I worked at a restaurant that was requiring th...     N\n",
       "2  @testpatternshow Went to a local restaurant, &...     Y\n",
       "3  OMG...went to pick up food at a restaurant tha...     Y\n",
       "4  Update from the Inner Cook County Resistance: ...     Y"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dataset[['Text','GT']]\n",
    "dataset.columns=['sentence','label']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sentence = dataset.sentence.apply(lambda x: text_prep(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label']=dataset['label'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1314, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['y', 'n'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1314, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.sample(frac = 1) \n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=dataset[:1100]\n",
    "dataset_test=dataset[1100:1314]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=dataset_train['sentence'].tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "x_train = tfidf_vectorizer.fit_transform(sent).toarray()\n",
    "\n",
    "pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl', 'wb'))\n",
    "\n",
    "y_train = list(dataset_train['label'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "pickle.dump(le, open('LabelEncoder.pkl', 'wb'))\n",
    "def encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return to_categorical(enc)\n",
    "\n",
    "def decode(le, one_hot):\n",
    "    dec = np.argmax(one_hot, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = encode(le, y_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='sigmoid',input_shape=(5779,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9556 - accuracy: 0.4898 - val_loss: 0.7455 - val_accuracy: 0.5273\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7469 - accuracy: 0.5057 - val_loss: 0.6898 - val_accuracy: 0.5273\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7277 - accuracy: 0.5080 - val_loss: 0.6916 - val_accuracy: 0.5273\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7251 - accuracy: 0.4977 - val_loss: 0.6889 - val_accuracy: 0.5273\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7045 - accuracy: 0.5386 - val_loss: 0.6876 - val_accuracy: 0.5409\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7038 - accuracy: 0.5250 - val_loss: 0.6909 - val_accuracy: 0.5273\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7074 - accuracy: 0.5273 - val_loss: 0.6864 - val_accuracy: 0.5273\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.5568 - val_loss: 0.6881 - val_accuracy: 0.5273\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7028 - accuracy: 0.5000 - val_loss: 0.6848 - val_accuracy: 0.6045\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.5716 - val_loss: 0.6869 - val_accuracy: 0.5273\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.5909 - val_loss: 0.6851 - val_accuracy: 0.5273\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.5886 - val_loss: 0.6824 - val_accuracy: 0.5364\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.5625 - val_loss: 0.6784 - val_accuracy: 0.5545\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6552 - accuracy: 0.5932 - val_loss: 0.6807 - val_accuracy: 0.5409\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6381 - accuracy: 0.6239 - val_loss: 0.6734 - val_accuracy: 0.6591\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6460 - accuracy: 0.6284 - val_loss: 0.6723 - val_accuracy: 0.5545\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6489 - val_loss: 0.6703 - val_accuracy: 0.5545\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.6636 - val_loss: 0.6606 - val_accuracy: 0.6545\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6039 - accuracy: 0.6557 - val_loss: 0.6601 - val_accuracy: 0.5682\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5859 - accuracy: 0.7114 - val_loss: 0.6513 - val_accuracy: 0.5955\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 32, epochs = 20, validation_split = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"tfidfmodel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"tfidf_model.h5\")\n",
    "\n",
    "print(\"Training Done\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('tfidf_vectorizer.pkl',\"rb\")\n",
    "tfidf_vectorizer = pickle.load(pickle_in)\n",
    "sent=dataset_test['sentence'].tolist()\n",
    "x_test = tfidf_vectorizer.transform(sent).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = list(dataset_test['label'])\n",
    "pickle_in = open('LabelEncoder.pkl',\"rb\")\n",
    "le_pickle = pickle.load(pickle_in)\n",
    "y_test = encode(le_pickle, y_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "json_file = open(\"tfidfmodel.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"tfidf_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 0s\n"
     ]
    }
   ],
   "source": [
    "predicts = loaded_model.predict(x_test, batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_preds = decode(le_pickle, predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = decode(le_pickle, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7102803738317757\n",
      "F1 Score :  0.6839953271028038\n",
      "confusion_matrix :  [[ 31  50]\n",
      " [ 12 121]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \",accuracy_score(y_test, y_preds))\n",
    "print(\"F1 Score : \",f1_score(y_test, y_preds,average='weighted'))\n",
    "print(\"confusion_matrix : \",confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index={}\n",
    "#reading embeddings\n",
    "with open('glove.6B.200d.txt','r',encoding='utf-8') as embd_file:\n",
    "    for line in embd_file:\n",
    "        val = line.split()\n",
    "        word = val[0]\n",
    "        cfs = np.asarray(val[1:], dtype='float32')\n",
    "        embeddings_index[word] = cfs\n",
    "\n",
    "pickle.dump(embeddings_index, open('embeddings_index.pkl', 'wb'))\n",
    "\n",
    "def avg_vectors(sentence):\n",
    "    sentence = [token for token in sentence.split() if  token in embeddings_index]\n",
    "    embedding_size = 200\n",
    "    vs = np.zeros(embedding_size)\n",
    "    sentence_length = len(sentence)\n",
    "    for word in sentence:\n",
    "        vs = np.add(vs, embeddings_index.get(word))\n",
    "    vs = np.divide(vs, sentence_length)\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec= dataset_train.sentence.apply(lambda x: avg_vectors(x))\n",
    "x_train=np.array(x_train_vec.values.tolist())\n",
    "\n",
    "y_train = list(dataset_train['label'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "pickle.dump(le, open('LabelEncoder.pkl', 'wb'))\n",
    "def encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return to_categorical(enc)\n",
    "\n",
    "def decode(le, one_hot):\n",
    "    dec = np.argmax(one_hot, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = encode(le, y_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7529 - accuracy: 0.5125 - val_loss: 0.6884 - val_accuracy: 0.5045\n",
      "Epoch 2/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.7441 - accuracy: 0.4886 - val_loss: 0.6775 - val_accuracy: 0.5818\n",
      "Epoch 3/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.7123 - accuracy: 0.5318 - val_loss: 0.6802 - val_accuracy: 0.5318\n",
      "Epoch 4/15\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.7001 - accuracy: 0.5386 - val_loss: 0.6717 - val_accuracy: 0.5773\n",
      "Epoch 5/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6872 - accuracy: 0.5670 - val_loss: 0.6603 - val_accuracy: 0.6136\n",
      "Epoch 6/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6795 - accuracy: 0.5716 - val_loss: 0.6550 - val_accuracy: 0.6227\n",
      "Epoch 7/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6588 - accuracy: 0.6261 - val_loss: 0.6555 - val_accuracy: 0.6000\n",
      "Epoch 8/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6514 - accuracy: 0.6136 - val_loss: 0.6474 - val_accuracy: 0.6136\n",
      "Epoch 9/15\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6394 - accuracy: 0.6227 - val_loss: 0.6425 - val_accuracy: 0.6409\n",
      "Epoch 10/15\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.6420 - val_loss: 0.6399 - val_accuracy: 0.6273\n",
      "Epoch 11/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6367 - accuracy: 0.6273 - val_loss: 0.6382 - val_accuracy: 0.6318\n",
      "Epoch 12/15\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.6170 - val_loss: 0.6430 - val_accuracy: 0.6455\n",
      "Epoch 13/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6270 - accuracy: 0.6330 - val_loss: 0.6547 - val_accuracy: 0.6273\n",
      "Epoch 14/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6288 - accuracy: 0.6443 - val_loss: 0.6353 - val_accuracy: 0.6273\n",
      "Epoch 15/15\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6148 - accuracy: 0.6614 - val_loss: 0.6579 - val_accuracy: 0.6273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc999ff86d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v = Sequential()\n",
    "model_w2v.add(Dense(128, activation='sigmoid',input_shape=(200,)))\n",
    "model_w2v.add(Dropout(0.3))\n",
    "model_w2v.add(Dense(64, activation='sigmoid'))\n",
    "model_w2v.add(Dropout(0.3))\n",
    "model_w2v.add(Dense(2, activation=\"softmax\"))\n",
    "model_w2v.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "model_w2v.fit(x_train, y_train, batch_size = 32, epochs = 15, validation_split = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model_json = model_w2v.to_json()\n",
    "with open(\"word2vecmodel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model_w2v.save_weights(\"word2vec_model.h5\")\n",
    "\n",
    "print(\"Training Done\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_in = open('embeddings_index.pkl',\"rb\")\n",
    "embeddings_index = pickle.load(pickle_in)\n",
    "\n",
    "def avg_vectors(sentence):\n",
    "    sentence = [token for token in sentence.split() if  token in embeddings_index]\n",
    "    embedding_size = 200\n",
    "    vs = np.zeros(embedding_size)\n",
    "    sentence_length = len(sentence)\n",
    "    for word in sentence:\n",
    "        vs = np.add(vs, embeddings_index.get(word))\n",
    "    vs = np.divide(vs, sentence_length)\n",
    "    return vs\n",
    "\n",
    "x_test_vec= dataset_test.sentence.apply(lambda x: avg_vectors(x))\n",
    "x_test=np.array(x_test_vec.values.tolist())\n",
    "\n",
    "\n",
    "def encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return to_categorical(enc)\n",
    "\n",
    "def decode(le, one_hot):\n",
    "    dec = np.argmax(one_hot, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_test = list(dataset_test['label'])\n",
    "pickle_in = open('LabelEncoder.pkl',\"rb\")\n",
    "le_pickle = pickle.load(pickle_in)\n",
    "y_test = encode(le_pickle, y_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "json_file_w2v = open(\"word2vecmodel.json\", 'r')\n",
    "loaded_model_json_w2v = json_file_w2v.read()\n",
    "json_file_w2v.close()\n",
    "loaded_model_w2v = model_from_json(loaded_model_json_w2v)\n",
    "loaded_model_w2v.load_weights(\"word2vec_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 0s\n",
      "Accuracy :  0.5747663551401869\n",
      "F1 Score :  0.5769496132412127\n",
      "confusion_matrix :  [[59 22]\n",
      " [69 64]]\n",
      "Testing Done\n",
      "Output Created\n"
     ]
    }
   ],
   "source": [
    "predict = loaded_model_w2v.predict(x_test, batch_size=32,verbose=2)\n",
    "y_test = decode(le_pickle, y_test)\n",
    "y_preds = decode(le_pickle, predict)\n",
    "print(\"Accuracy : \",accuracy_score(y_test, y_preds))\n",
    "print(\"F1 Score : \",f1_score(y_test, y_preds,average='weighted'))\n",
    "print(\"confusion_matrix : \",confusion_matrix(y_test, y_preds))\n",
    "\n",
    "print(\"Testing Done\")\n",
    "print(\"Output Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - Using ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "0.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(dataset_train['label'])\n",
    "x_train = list(dataset_train['sentence'])\n",
    "\n",
    "lb = preprocessing.LabelEncoder()\n",
    "lb.fit(y_train)\n",
    "pickle.dump(lb, open('LabelEncoder.pkl', 'wb'))\n",
    "\n",
    "def encd(lb, labels):\n",
    "    enc = lb.transform(labels)\n",
    "    return to_categorical(enc)\n",
    "\n",
    "def decd(lb, one_hot):\n",
    "    dec = np.argmax(one_hot, axis=1)\n",
    "    return lb.inverse_transform(dec)\n",
    "\n",
    "y_train = encd(lb, y_train)\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://tfhub.dev/google/elmo/2\"\n",
    "embed = hub.Module(url)\n",
    "\n",
    "def elmoembd(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "embedding = Lambda(elmoembd, output_shape=(1024, ))(input_text)\n",
    "dense1 = Dense(128, activation='relu')(embedding)\n",
    "dense2= Dropout(0.5)(dense1)\n",
    "dense3 = Dense(256, activation='relu')(dense2)\n",
    "dense4= Dropout(0.3)(dense3)\n",
    "pred = Dense(2, activation='softmax')(dense4)\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 880 samples, validate on 220 samples\n",
      "Epoch 1/5\n",
      "880/880 [==============================] - ETA: 0s - loss: 0.7076 - accuracy: 0.5330WARNING:tensorflow:From /Users/ssabu2/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ssabu2/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 [==============================] - 64s 73ms/sample - loss: 0.7076 - accuracy: 0.5330 - val_loss: 0.6600 - val_accuracy: 0.6091\n",
      "Epoch 2/5\n",
      "880/880 [==============================] - 66s 75ms/sample - loss: 0.6855 - accuracy: 0.5568 - val_loss: 0.6456 - val_accuracy: 0.6136\n",
      "Epoch 3/5\n",
      "880/880 [==============================] - 60s 68ms/sample - loss: 0.6256 - accuracy: 0.6500 - val_loss: 0.6284 - val_accuracy: 0.6227\n",
      "Epoch 4/5\n",
      "880/880 [==============================] - 58s 66ms/sample - loss: 0.6134 - accuracy: 0.6636 - val_loss: 0.6308 - val_accuracy: 0.6364\n",
      "Epoch 5/5\n",
      "880/880 [==============================] - 60s 68ms/sample - loss: 0.5927 - accuracy: 0.6932 - val_loss: 0.6239 - val_accuracy: 0.6500\n",
      "Training Done\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.compat.v1.keras.backend.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    history = model.fit(x_train, y_train, epochs=5, batch_size=32,verbose=1, validation_split=0.2)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"elmomodel.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"elmo_model.h5\")\n",
    "\n",
    "print(\"Training Done\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = list(dataset_test['label'])\n",
    "x_test = list(dataset_test['sentence'])\n",
    "\n",
    "pickle_in = open('LabelEncoder.pkl',\"rb\")\n",
    "le_pickle = pickle.load(pickle_in)\n",
    "\n",
    "def encode(lb, labels):\n",
    "    enc = lb.transform(labels)\n",
    "    return to_categorical(enc)\n",
    "\n",
    "def decode(lb, one_hot):\n",
    "    dec = np.argmax(one_hot, axis=1)\n",
    "    return lb.inverse_transform(dec)\n",
    "\n",
    "y_test = encode(le_pickle, y_test)\n",
    "\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6448598130841121\n",
      "F1 Score :  0.6322674298190949\n",
      "confusion_matrix :  [[ 33  48]\n",
      " [ 28 105]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.compat.v1.keras.backend.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    json_file = open('elmomodel.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"elmo_model.h5\")\n",
    "    predicts = loaded_model.predict(x_test, batch_size=32,verbose=2)\n",
    "    \n",
    "y_test = decode(le_pickle, y_test)\n",
    "y_preds = decode(le_pickle, predicts)\n",
    "\n",
    "print(\"Accuracy : \",accuracy_score(y_test, y_preds))\n",
    "print(\"F1 Score : \",f1_score(y_test, y_preds,average='weighted'))\n",
    "print(\"confusion_matrix : \",confusion_matrix(y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
